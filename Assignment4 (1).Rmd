---
title: "Assignment 4 - Applying meta-analytic priors"
author: "Riccardo Fusaroli"
date: "3/16/2018"
output: html_document
---

```{r setup, include=FALSE}
knitr::opts_chunk$set(echo = TRUE)

getwd()
locpath= getwd()
setwd(locpath)

library(pacman)
p_load(ggplot2, rethinking, readxl, brms, metafor, tidyverse, lme4, Rstan)

d = read_excel("Assignment4PitchDatav2.xlsx")
d_meta = read_excel("Assignment4MetaData.xlsx")

```

## Assignment 4

In this assignment we do the following:
- we reproduce the meta-analysis of pitch SD from last semester in a Bayesian framework
- we reproduce the pitch SD in schizophrenia analysis from last semester using both a conservative and a meta-analytic prior
- we assess the difference in model quality and estimates using the two priors.

The questions you need to answer are: What are the consequences of using a meta-analytic prior? Evaluate the models with conservative and meta-analytic priors. Discuss the effects on estimates. Discuss the effects on model quality. Discuss the role that meta-analytic priors should have in scientific practice. Should we systematically use them? Do they have drawbacks? Should we use them to complement more conservative approaches? How does the use of meta-analytic priors you suggest reflect the skeptical and cumulative nature of science?

### Step by step suggestions

Step 1: Reproduce the meta-analysis of pitch sd from previous studies of voice in schizophrenia
- the data is available as Assignment4MetaData.xlsx
- Effect size (cohen's d), sd and variance are already calculated (you're welcome!)
- Since we're only interested in getting a meta-analytic effect size, let's take a shortcut and use bromance magic (brms): https://mvuorre.github.io/post/2016/2016-09-29-bayesian-meta-analysis/

```{r}

# m <- BRM(MeanES|SE(SdES)) ~ 1 + (1|StudyREF), 
#         # Prior = # the computer will compute the best prior by itself 
#         Data = d_meta
#         Cores = 2 # has to do with the computer, we'll run the model(s) on two processors/cores??
#         Chain = 2 # again somehitng woth the computer and maybe 2 processes
#         ITER = 2000
# # Dealing with the Cohens d
# standardize over all wtudies to top_n(scale)
# mean group 1 - minus - studies mean of model 2 = pooled SD
# the more unsertain - delete - make it balande - the storinger/dloset to a normai voidse, the 
# 


model <- brm(MeanES|se(SdES) ~ 1 + (1|StudyRef), data = d_meta, prior = NULL, chains = 2, iter = 2000, cores = 2)
summary(model)

# making a forestplot to visualize data
m1 = rma(yi = MeanES, vi = SdES, data = d_meta, slab = StudyRef)
forest(m1)


# to do:
# forest plot 
# look at estimates and error (does the meta analytic effect look gaussian?)
# this is a simulation (?)
# the numbers are our uncertainties (-0.55 and 0.25)
# we want to eliminate the need to use random effects (we only want 1 data point per participant)
# - idea: group by id and summarise mean ?? (then we have a mean for each participant)

```


BRM is instead of the glmer
outcome: mean effc´effect size 
=z
meanES (mean effect size)
MeanEs | SdEs 



Step 2: Prepare the pitch SD data from last year
- the data is available as Assignment4PitchData.csv (thanks Celine)
- We do not know how to build random effects, yet. So we need to simplify the dataset to avoid interdependence between datapoint: How?
- Also, let's standardize the data, so that they are compatible with our meta-analytic prior (Cohen's d is measured in SDs).

```{r}
#standardize the data
library(dplyr)
d_s = scale(d, center = TRUE, scale = TRUE)
d_s = as.data.frame(d_s)

#take the mean of pitch SD for each participant, so we don't have to use random effects in our model (from 10 datapoints to 1)
Unique_PitchSD = d_s %>% 
  group_by(ID) %>% 
  summarise(Unique_PitchSD = mean(PitchSD))
  
d_s = merge(d_s,Unique_PitchSD)

# making a data frame with only unstandardised ID and diagnosis and unique pitch SD 
data = cbind(d$ID, d$diagnosis, d_s$Unique_PitchSD)

# rename column names
colnames(data) <- c("ID", "diagnosis", "Unique_PitchSD")

# removing duplicated rows
data = data[!duplicated(data), ]

# making it a data frame 
data = as.data.frame(data)
```


Step 3: Build a regression model predicting Pitch SD from Diagnosis.
- how is the outcome distributed? (likelihood function)
- how are the parameters of the likelihood distribution distributed? Which predictors should they be conditioned on?
- use a skeptical/conservative prior for the effects of diagnosis. Remember you'll need to motivate it.
- Describe and plot the estimates. Evaluate model quality

```{r}


# Build a regression model predicting Pitch SD from Diagnosis (w the brm function)
model <- brm(Unique_PitchSD ~ 1 + diagnosis, data = data, prior = NULL, chains = 2, iter = 2000, cores = 2)
summary(model)

# Build a regression model predicting Pitch SD from Diagnosis (w the MAP function)
# posterior <- likelihood * prior 

# Pi ~ Normal(µ,sigma)   = likelihood
# µ ~ Normal(mean, SD)   = µ prior
# sigma ~ Uniform(0,50)  = sigma prior 

#notes from class
# #use standardized data
# PitchSD ~ Normal(mu, sigma) #because sampling across participants and doing mean. the samples means will be normally distributed
# mu = a + b*Diagnosis #because linear model
# a ~ Normal(0, 1) #0 and 1 because the data is standardized. can vary just as much as data
# b ~ Normal(0, 1) #the beta is telling us about the difference between the two diagnosis, so it is not binomial
# sigma ~ Cauchy(0, 2)#the variance. given the expected mean, how much do we expect the actual value to vary on average. what's the expected error. has to be continuous number above 0 (because you can't have a negative value of error). 2 is about how thick is the long tail. the 0 is where it starts.
# #can also do (not neccessary for the assignment:
# log(sigma) = as + bs*Diagnosis
# 
# #how to do random effects (also not neccessary):
# mu = a[participant] + b[participant]*Diagnosis #means look out, it should be one value for each alpha and beta - each participant has a different value
# a[participant] ~ Normal(a, 1)
# a ~ Normal(0, 1)

#model made with map and not conservative
Pitchmodel <- rethinking::map(
  alist(
    Unique_PitchSD ~ dnorm(mu, sigma),
    mu <- a + bD * diagnosis,
    a ~ dnorm(0, 1),
    bD ~ dnorm(0, 1),
    sigma ~ dcauchy(0, 2)
  ),
  data = data
)

precis(Pitchmodel)
precis_plot(precis(Pitchmodel))

#model made with map2stan and with conservative priors
Pitchmodel2 <- map2stan(
  alist(
    Unique_PitchSD ~ dnorm(mu, sigma),
    mu <- a + bD * diagnosis,
    a ~ dnorm(0, 0.1),
    bD ~ dnorm(0, 0.1),
    sigma ~ dcauchy(0, 4)
  ),
  data = data,
  chains = 4, cores = 2, iter = 5000, warmup = 3000
)

precis(Pitchmodel2)
show(Pitchmodel2)
post <- extract.samples(Pitchmodel2)
str(post)
pairs(post)
pairs(Pitchmodel2)
precis_plot(precis(Pitchmodel2))

#notes to self: we care most about the beta value because it tells us about the difference between the two groups. the priors (0, 1) are large and we want to be more skeptic. we could do (0, 0.2) -> we expect diff between the two groups to vary from -0.6 to +0.6 (because it is 0.2 = 20% of -3 and 3, which is where the values are (?)). for skeptical prior (0, 0.1) is good - not too skeptical.




```
from the pairs plot of the model we can see that alpha and beta for diagnosis are negatively correlated, while sigma is not very correlated with either.
We chose a prior for the alpha and beta to be (0, 0.1) because we expect the difference between the groups to vary with 10%. We think this is a conservative prior. 

```{r}
#errors is this?!
# call link without specifying new data
# so it uses original data
mu <- link(Pitchmodel2)
# summarize samples across cases
mu.mean <- apply( mu , 2 , mean )
mu.PI <- apply( mu , 2 , PI )
# simulate observations
# again no new data, so uses original data
model.sim <- sim( fit = Pitchmodel2 , n=1e4  )
model.PI <- apply( model.sim , 2 , PI  )

# plot predictions against observed 
plot( mu.mean ~ data$Unique_PitchSD , col=rangi2 , ylim=range(mu.PI) ,
    xlab="Observed Pitch SD" , ylab="Predicted Pitch SD" )
abline( a=0 , b=1 , lty=2 )
for ( i in 1:nrow(data) )
    lines( rep(data$Unique_PitchSD[i],2) , c(mu.PI[1,i],mu.PI[2,i]) ,
        col=rangi2 )


```


Step 4: Now re-run the model with the meta-analytic prior
- Describe and plot the estimates. Evaluate model quality

```{r}
#Need to find new priors
#We have these values from our metaanalysis model: 
#int = -0.54     se(int) = 0.25       sd(int) = 0.72     se(sd(int)) = 0.23
a #estimated pitch SD from the controls, so we don't do anything because we don't have any data on this from the meta-analysis
b ~ Normal(-0.54, 0.25) #from intercept and SE of intercept. SD of intercept could also be used instead of SE but don't do that here.
sigma #estimated error. don't change it because we dont have data on this from the meta-analysis
#now we are talking about subjects and not studies.

Pitchmodel3 <- map2stan(
  alist(
    Unique_PitchSD ~ dnorm(mu, sigma),
    mu <- a + bD * diagnosis,
    a ~ dnorm(0, 0.1),
    bD ~ dnorm(-0.54, 0.25),
    sigma ~ dcauchy(0, 4)
  ),
  data = data,
  chains = 4, cores = 2, iter = 5000, warmup = 3000
)

precis(Pitchmodel2)
pairs(Pitchmodel2)
precis_plot(precis(Pitchmodel3))
```


Step 5: Compare the models
- Plot priors and posteriors of the diagnosis effect in both models
- Compare posteriors between the two models
- Compare their relative distance from truth (WAIC)
- Discuss how they compare and whether any of them is best.

Step 6: Prepare a nice write up of the analysis and answer the questions at the top.

Optional step 7: how skeptical should a prior be?
- Try different levels of skepticism and compare them using WAIC.

Optional step 8: Include other predictors
- Do age, gender and education improve the model?
- Should they be main effects or interactions?

Optional step 9: Bromance magic.
- explore the bromance code below including random effects (by default with weakly informative priors)
- learn how to change the prior
- explore effects of trial, age, gender, including the appropriate random slopes
- compare the models you created using WAIC and posterior predictive check (pp_check())


```{r}

brm_out <- brm(PitchSD ~ 1 + Diagnosis  +(1|ID_unique/Study), # Outcome as a function of the predictors as in lme4. 
               data=Data, # Define the data
               family=gaussian(), # Define the family. 
               iter = 5000, warmup = 2000, cores = 4)
summary(brm_out1)
plot(brm_out1)

```

